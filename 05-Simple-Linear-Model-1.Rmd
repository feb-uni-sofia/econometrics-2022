---
title: "Week 5"
author: "Boyko Amarov"
date: "4/19/2021"
output:
  bookdown::html_document2: default
---

```{r setup, include=FALSE}
## Warning: to compile the notes you need the "bookdown" and the "broom" packages. Install them by
## running install.packages, see the commented lines below

# install.packages(c("patchwork", "bookdown", "broom"))

library(tidyverse)
library(broom)
library(patchwork)

kids <- read_csv(
  "https://raw.githubusercontent.com/feb-uni-sofia/econometrics2020-solutions/master/data/childiq.csv") %>% 
  select(kid_score, mom_hs) %>%
   mutate(
    mom_hs_string = ifelse(mom_hs == 1, "With HS", "Without HS")
  )
```

# Linear regression model with a single categorical predictor


Variables description:


- `kid_score`: (numeric) Kidâ€™s IQ score.
- `mom_hs` (binary): 1 if the mother has finished high school, 0 otherwise.


Two groups of kids: the ones whose mother had a high school degree (`mom_hs = 1`) and the rest (`mom_hs = 0`).

Question: are this two groups different with respect to IQ (as measured by `kid_score`).


Research hypothesis: children of mothers without a high school degree have 
lower IQ on average compared to children of mothers with a high school degree.


```{r}
kids %>%
  ## Group the data according to the educational status of the mothers
  group_by(mom_hs) %>%
  ## Compute the average IQ score in each group
  summarise(
    average_IQ = mean(kid_score)
  )
```



```{r}
kids %>%
  ggplot(aes(
    x = mom_hs_string,
    y = kid_score,
    color = mom_hs_string
    )
  ) +
  geom_point(position = "jitter") +
  geom_boxplot(alpha = 0.5)
```

Formulated as a linear regression model:

$$
i = 1,\ldots, n = 434\\
y_i: \text{Kid IQ score} \\
x_i \in \{0, 1\}
$$

$$
y_i = \beta_0 + \beta_1 x_i + u_i, u_i \sim N(0, \sigma^2)
$$


```{r}
fit <- lm(kid_score ~ 1 + mom_hs, data = kids)
fit
```


$$
\hat{y}_i = 77.55 + 11.77 x_i
$$
```{r}
mean(kids$kid_score)
```


```{r}
kids %>%
  group_by(mom_hs) %>%
  summarise(
    Average_IQ = mean(kid_score)
  )
```



The OLS estimate for the intercept corresponds to the average $y$ in the group with $x = 0$ (77.55) and the estimate for the slope equals to the difference of the averages in the $x = 1$ and $ x = 0$ group (89.31965 - 77.54839 = 11.77).

So the estimated difference of the averages in the two groups (with HS and without HS) is 11.77.


$$
E(y_i) = \beta_0 + \beta_1 x_i\\
\begin{align}
E(y_i) = 
\begin{cases} 
\mu_1 = \beta_0 + \beta_1 & \text{if } x_i = 1 \text{} \\
\mu_0 = \beta_0 & \text{if } x_i = 0 \text{ }
\end{cases}   
\end{align}\\
\implies\\
\beta_1 = \mu_1 - \mu_0
$$
$$
\hat{\beta}_0 = \hat{\mu}_0 = \bar{y}_{x = 0}\\
\hat{\beta}_1 = \hat{\mu}_1 - \hat{\mu}_0 = \bar{y}_{x = 1} - \bar{y}_{x = 0}
$$

In our example

$$
\hat{\beta}_0 = 77.54839\\
\hat{\beta}_1 = 89.31965 - 77.54839 = 11.77126
$$

Assume that the kids were selected at random from the population of children in the USA (at the time of the survey).


Assume that the kids were selected at random from the population of all kids in the USA.


There are a lot of samples that we could have observed, but we have selected only _one_ sample!

Assumption about all the possible samples. Before that we formalise our comparison between the two groups
in terms of a linear regression model.


## Simulation

Let us take the linear regression model and let us _assume_ that the random term $e_i$ follows a normal distribution with zero mean $\mu = 0$ and some standard deviation $\sigma$ (or equivalently variance $\sigma^2$).

$$
y_i = \beta_0 + \beta_1 x_i + u_i\\
\text{Assume } u_i \sim N(0, \sigma^2)
$$

Assume that we know the function that describes the population of children is:

<!-- (\#eq:reg-pop) -->
$$
\begin{align}
  y_i = 77.55 + 11.77 x_i + u_i, u_i \sim N(0, 19.85^2)
\end{align}
$$

We would like to study the way the estimates for $\beta_0$ and $\beta_1$ vary from sample to sample. For that purpose we will generate $R = 2000$ samples from the population described by equation \@ref(eq:reg-pop). The we will estimate the regression model for each of these 2000 samples. What follows is a lot of rather technical details that
we need to generate the values for the samples and arrange these values in a way that is convenient for plotting and analysis.

We begin by introducing the `group_by` function and we'll use it to compute the sample group means for the two groups defined by `mom_hs`.


Next we construct a data set using `expand_grid` to replicate the `mom_hs` column $R = 2000$ times.

```{r}
kids$mom_hs
```

```{r}
## Number of simulated samples
## Construct a data set with mom_hs repeated R = 2000 times
## Examine the contents of sim_df to see what expand_grid returns

sim_df <- expand_grid(
    R = 1:2000, ## sequence of all integers between 1 and 3
    mom_hs = kids$mom_hs
  )
```


After that we calculate the (simulated) IQ score for every child in every sample (a total of $2000 \times 434$ values) by using equation \@ref{eq:pop-reg}. For the value of the random term we select a value from the normal distribution with mean 0 and standard deviation $\sigma = 20.41069$ (we will turn to the estimation of $\sigma$ later).

```{r}
## Fixes the random numbers sequence so that you can reproduce the 
## random draws
# set.seed(4234)

sim_df <- sim_df %>%
  mutate(
    ## Compute a simulated IQ score for each kid according to our estimated regression equation
    ## rnorm adds a value selected at random from a normal distribution with mean = 0 and standard
    ## deviation (sigma) = 19.85 (that we estimated from the sample)
    ## rnorm requires to know the number of values to be selected at random
    ## n(): the number of observations in the dataset
    u = rnorm(n(), mean = 0, sd = 19.85),
    kid_score = 77.55 + 11.77 * mom_hs + u
  )
```

The last step completes the simulation of our 2000 samples and we can start estimating the regression model in each sample. For that we group the simulation data set `sim_df` by the simulation number (variable $R$) and run the regression model in every group. The `tidy` function serves to transform the output of `lm` into a format that can easily fit into a rectangular table.


```{r}
## Estimate the OLS coefficients with the data from the first sample of children
sample1 <- sim_df %>% filter(R == 1)
head(sample1)
```

```{r}
lm(kid_score ~ 1 + mom_hs, data = sample1)
```

```{r}
sample2 <- sim_df %>% filter(R == 2)
head(sample2)
```

```{r}
lm(kid_score ~ mom_hs, data = sample2)
```




```{r}
## Code for illustration purposes only

sim_coeff <- sim_df %>%
  group_by(R) %>%
  ## The tidy function reformats the output of lm so that it can fit in a data frame
  do(tidy(lm(kid_score ~ mom_hs, data = .))) %>%
  select(R, term, estimate)
```

The data set `sim_coeff` now contains estimated coefficients ($\hat{\alpha}$ and $\hat{\beta}$) for every sample. To plot the distribution of $\hat{\beta}}$ we filter the data set so that we keep only the raw where `term == "mom_hs"` (i.e our estimates for $\beta$).

```{r}
slopes <- sim_coeff %>% filter(term == "mom_hs")
```


```{r}
slopes %>%
  ggplot(aes(x = estimate)) + 
  geom_point(
    aes(y = 0),
    position = "jitter", 
    size = 1/2,
    alpha = 0.5
    ) +
    geom_boxplot(alpha = 0.5) +
  labs(
    x = "Slope estimate",
    title = "Distribution of slope estimates (over 2000 samples)",
    y = ""
  ) +
  geom_density(color = "steelblue")
 #  geom_vline(xintercept = 11.77, color = "red") +
 #  xlim(c(0, 21))
```
```{r}
summary(fit)
```

