---
title: "Einfaches Lineares Modell (1)"
author: "Boyko Amarov"
date: "4/18/2022"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(broom)
library(patchwork)

## install.packages(c("bookdown", "patchwork", "broom"))

kids <- read_csv(
  "https://raw.githubusercontent.com/feb-uni-sofia/econometrics2020-solutions/master/data/childiq.csv") %>%
  select(kid_score, mom_hs) %>%
  mutate(
    mom_hs_string = ifelse(mom_hs == 1, "Mit Abitur", "Ohne Abitur")
  )
```

## Lineares Regressionsmodell mit einem kategoriallen Prädiktor 

Variablenbeschreibung:

- `kid_score`: (numeric) IQ Test (Punkte)
- `mom_hs` (binary): 1 Mutter hat Abitur, 0 Mutter hat keine Abitur

Frage:
Gibt es Unterschiede zwischen den Intelligenztestergebnissen von Kindern mit Müttern mit/ohne Abitur.


```{r}
kids %>%
  ggplot(
    aes(
      x = mom_hs_string,  
      y = kid_score,
      color = mom_hs_string
      )
    ) +
    geom_point(position = "jitter") +
    geom_boxplot(alpha = 0.5) +
      labs(
      x = "Gruppenzugehörigkeit",
      y = "Erzielte Punktzahl",
      color = "Gruppenzugehörigkeit"
    )
```

$$
i = 1,\ldots,n = 434 \text{ Kinder}\\
y_i: \text{ Testergebnis von Kind } i\\
x_i \in \{0, 1\}: \text{ Gruppenzugehörigkeit von Kind i (1: Mutter mit Abitur/0: Mutter ohne Abitur) }\\
$$

$$
y_i = \beta_0 + \beta_1 x_i + u_i, \quad E(u_i) = 0
$$

$$
E(y_i) = \beta_0 + \beta_1 x_i\\
\begin{align}
E(y_i) = 
\begin{cases} 
\mu_1 = \beta_0 + \beta_1 & \text{falls } x_i = 1 \text{ Erwartetes Testergebnis für die Gruppe mit Abitur (der Mutter)} \\
\mu_0 = \beta_0 & \text{falls } x_i = 0 \text{ Erwartetes Testergebnis für die Gruppe ohne Abitur (der Mutter)}
\end{cases}   
\end{align}\\
\implies\\
\beta_1 = \mu_1 - \mu_0
$$
$\beta_0$ ist einfach das erwartete Testergebnis für die Gruppe ohne Abitur (der Mutter). $\beta_1$ ist einfach die Differenz zwischen den erwarteten Testergebnissen der zwei Gruppen.


```{r}
lm(kid_score ~ 1 + mom_hs, data = kids)
```

Wir berechnen die Gruppenmittelwerte:

```{r}
mean(kids$kid_score)
```


```{r}
kids %>%
  group_by(mom_hs) %>%
  summarise(
    Average_IQ = mean(kid_score)
  )
```


Man kan zeigen, daß die KQ-Methode auf die folgende Schätzung für $\beta_0$ und $\beta_1$ führt:

$$
\hat{\beta}_0 = \hat{\mu}_0 = \bar{y}_{x = 0}\\
\hat{\beta}_1 = \hat{\mu}_1 - \hat{\mu}_0 = \bar{y}_{x = 1} - \bar{y}_{x = 0}
$$

$$
\hat{\beta_0} = 77.55 \text{ Mitterwert der Testergebnise in der Gruppe ohne Abitur (Mutter)}\\
\hat{\beta} = 11.77 = 89.32 - 77.55 \text{ Differenz der zwei Gruppenmitterwerte} \\
\hat{y}_i =  \hat{\beta}_0 + \hat{\beta}_1 x_i\\
\hat{y}_i = 77.55 + 11.77 x_i \text{ geschätzte Gleichung}
$$
Das geschätzte Testergebnis von Kindern (Mutter ohne Abitur, $x = 0$) is gleich 77.55 (Punkte). Das geschätzte Testergebnis von Kindern (Mutter mit Abitur, $x = 1$) is um 11.77 Punkte höher als das erwartete Testregebnis der anderen Groupe (Mutter ohne Abitur, $x = 0$).

$$
89.32 - 77.55 = 11.77
$$


## Simulation

Let us take the linear regression model and let us _assume_ that the random term $e_i$ follows a normal distribution with zero mean $\mu = 0$ and some standard deviation $\sigma$ (or equivalently variance $\sigma^2$).

$$
y_i = \beta_0 + \beta_1 x_i + u_i\\
\text{Assume } u_i \sim N(0, \sigma^2)
$$

Assume that we know the function that describes the population of children is:

<!-- (\#eq:reg-pop) -->
$$
\begin{align}
  y_i = 77.55 + 11.77 x_i + u_i, u_i \sim N(0, 19.85^2)
\end{align}
$$

We would like to study the way the estimates for $\beta_0$ and $\beta_1$ vary from sample to sample. For that purpose we will generate $R = 2000$ samples from the population described by equation \@ref(eq:reg-pop). The we will estimate the regression model for each of these 2000 samples. What follows is a lot of rather technical details that
we need to generate the values for the samples and arrange these values in a way that is convenient for plotting and analysis.

We begin by introducing the `group_by` function and we'll use it to compute the sample group means for the two groups defined by `mom_hs`.


Next we construct a data set using `expand_grid` to replicate the `mom_hs` column $R = 2000$ times.

```{r}
# kids$mom_hs
```

```{r}
## Number of simulated samples
## Construct a data set with mom_hs repeated R = 2000 times
## Examine the contents of sim_df to see what expand_grid returns

sim_df <- expand_grid(
    R = 1:2000, ## sequence of all integers between 1 and 3
    mom_hs = kids$mom_hs
  )
```


After that we calculate the (simulated) IQ score for every child in every sample (a total of $2000 \times 434$ values) by using equation \@ref{eq:pop-reg}. For the value of the random term we select a value from the normal distribution with mean 0 and standard deviation $\sigma = 20.41069$ (we will turn to the estimation of $\sigma$ later).

```{r}
## Fixes the random numbers sequence so that you can reproduce the 
## random draws
# set.seed(4234)

sim_df <- sim_df %>%
  mutate(
    ## Compute a simulated IQ score for each kid according to our estimated regression equation
    ## rnorm adds a value selected at random from a normal distribution with mean = 0 and standard
    ## deviation (sigma) = 19.85 (that we estimated from the sample)
    ## rnorm requires to know the number of values to be selected at random
    ## n(): the number of observations in the dataset
    u = rnorm(n(), mean = 0, sd = 19.85),
    kid_score = 77.55 + 11.77 * mom_hs + u
  )
```

The last step completes the simulation of our 2000 samples and we can start estimating the regression model in each sample. For that we group the simulation data set `sim_df` by the simulation number (variable $R$) and run the regression model in every group. The `tidy` function serves to transform the output of `lm` into a format that can easily fit into a rectangular table.


```{r}
## Estimate the OLS coefficients with the data from the first sample of children
sample1 <- sim_df %>% filter(R == 1)
head(sample1)
```

```{r}
lm(kid_score ~ 1 + mom_hs, data = sample1)
```

```{r}
sample2 <- sim_df %>% filter(R == 2)
head(sample2)
```

```{r}
lm(kid_score ~ mom_hs, data = sample2)
```




```{r}
## Code for illustration purposes only

sim_coeff <- sim_df %>%
  group_by(R) %>%
  ## The tidy function reformats the output of lm so that it can fit in a data frame
  do(tidy(lm(kid_score ~ mom_hs, data = .))) %>%
  select(R, term, estimate, std.error, statistic)
```

The data set `sim_coeff` now contains estimated coefficients ($\hat{\alpha}$ and $\hat{\beta}$) for every sample. To plot the distribution of $\hat{\beta}}$ we filter the data set so that we keep only the raw where `term == "mom_hs"` (i.e our estimates for $\beta$).

```{r}
slopes <- sim_coeff %>% filter(term == "mom_hs")
```


```{r}
slopes %>%
  ggplot(aes(x = estimate)) + 
  geom_point(
    aes(y = 0),
    position = "jitter", 
    size = 1/2,
    alpha = 0.5
    ) +
    geom_boxplot(alpha = 0.5) +
  labs(
    x = "Geschätzte Steigungskoeffizienten",
    title = "Verteilung (2000 Stichproben)",
    y = ""
  ) +
  geom_density(color = "steelblue") +
  geom_vline(xintercept = 11.77, color = "red")
 #  xlim(c(0, 21))
```

## Summary von lm

```{r}
fit <- lm(kid_score ~ 1 + mom_hs, data = kids)
summary(fit)
```

Der Standardfehler von $\beta_1$ ist die Standardabweichung der Verteilung der Schätzungen für $\beta_1$.

Was ist die Standardabweichung einer Verteilung:

Berechnung der Standardabweichung von $\hat{\beta}_1$ in der Simulation:

```{r}
## sd: s: standard, d: deviation
sd(slopes$estimate)
```


Difference between two distributions with a low (sd = 1) and a high (sd = 2) standard deviation.

```{r}
sim_std_dev <- expand_grid(
  sd = c(1, 2),
  R = 1:1000,
) %>%
  mutate(
    label = ifelse(sd == 1, "StdAbw = 1", "StdAbw = 2"),
    u = rnorm(n(), mean = 0, sd = sd)
  )

sim_std_dev %>%
  ggplot(aes(x = u)) +
    geom_histogram() +
    facet_wrap(~label) +
    labs(
      x = "Abweichung vom Ziel"
    )
    
```




```{r}
summary(fit)
```


## Hypothesentests ($H_0$ ist wahr)

$$
H_0: \beta_1 = 11.77\\
H_1: \beta_1 \neq 11.77
$$

t-test

$$
\text{t-Statistik} = \frac{\hat{\beta}_1 - 11.77}{SE(\hat{\beta}_1)}
$$

$$
t = \frac{11.77 - 11.77}{2.322} = 0
$$
Große Werte der t-Statistik sprechen gegen die Nullhypothese. Kleine Werte der t-Statistik deuten
darauf hin, daß die Daten mit der Nullhypothese kompatibel sind (die Daten enthalten nicht genug Information, um die Nullhypothese zu verwerfen). Wir erwarten, daß die t-Statistik klein ist, wenn 
die Nyllhypothese wahr ist.

```{r}
slopes <- slopes %>%
  mutate(
    t_statistic = (estimate - 11.77) / std.error
  )
```


```{r}
slopes %>%
  ggplot(aes(x = t_statistic)) + 
    geom_point(
      aes(y = 0),
      position = "jitter",
      size = 1/2,
      alpha = 0.5
      ) +
      geom_boxplot(alpha = 0.5) +
    labs(
      x = "Wert der t-statistic",
      title = "Verteilung der t-Statistik (H_0: beta_1 = 11.77, wahr)",
      y = ""
    ) +
    geom_density(color = "steelblue") +
    geom_vline(xintercept = 0, color = "red") +
    geom_vline(xintercept = c(-2, 2), color = "steelblue", lty = 2) +
    geom_vline(xintercept = c(-3, 3), color = "firebrick", lty = 2) +
    scale_x_continuous(breaks = c(-3, -2, 0, 2, 3))
   #  xlim(c(0, 21))
```

The real coefficient equals 11.77 (it is known, because we choose it for the simulation).

Lets assume a rule that we reject the null hypothesis $H_0: \beta_1 = 0$ vs. $H_1: \beta_1 \neq 0$ if the value of the t-statistic is less than -2 or greater than +2.

In how many samples will we wrongly reject the null hypothesis using this rule?

```{r}
testing_1 <- slopes %>%
  mutate(
    ## Logisches ODER: |
    fehlentscheidung_blau = t_statistic < -2 | t_statistic > 2,
    fehlentscheidung_rot = t_statistic < -3 | t_statistic > 3
  )
## Anteil von TRUE Werten
mean(testing_1$fehlentscheidung_blau)
mean(testing_1$fehlentscheidung_rot)
```


## Hypothesis testing


$$
H_0: \beta_1 = 0\\
H_1: \beta_1 \neq 0
$$
Diese Nullhypothese bedeutet, daß es keinen Unterschied (die mittleren Intelligenzkoeffizienten der Kinder sind gleich) zwischen den zwei Gruppen
in der Population gibt.

$$
y_i = \beta_0 + \beta_1 x_i + u_i
$$

If $H_0$ is true, the the model is simply

$$
y_i = \beta_0 + u_i
$$


t-test

$$
\text{t-statistic} = \frac{\hat{\beta}_1 - 0}{SE(\hat{\beta}_1)}
$$
The value of the t-statistic is small when the estimate for the coefficient is close to the value under the null hypothesis. The value of the t-statistic will be small, if the standard error of the estimator is high.

$$
t = \frac{11.77 - 0}{2.322} = 5.069
$$

Compute the value of the t-statistic for all samples in the simulation (and compare it to the value of the `statistic` column in the `sim_coef` dataset)

```{r}
slopes <- slopes %>%
  mutate(
    t_statistic0 = (estimate - 0) / std.error
  )
```



```{r}
slopes %>%
  ggplot(aes(x = t_statistic0)) + 
    geom_point(
      aes(y = 0),
      position = "jitter",
      size = 1/2,
      alpha = 0.5
      ) +
      geom_boxplot(alpha = 0.5) +
    labs(
      x = "Value of the t-statistic",
      title = "Distribution of t-statistic, beta_1 = 0 (false)",
      y = ""
    ) +
    geom_density(color = "steelblue") +
    geom_vline(xintercept = 0, color = "red") +
    geom_vline(xintercept = c(-2, 2), color = "steelblue", lty = 2) +
    geom_vline(xintercept = c(-3, 3), color = "firebrick", lty = 2) +
    xlim(c(-4, 8)) +
    scale_x_continuous(breaks = c(-3, -2, 0, 2, 3))
```

Die Nullhypothese $\beta_1 = 0$ ist falsch, deswegen werden wir eine Fehlentscheidung
treffen, wenn wir sie _nicht_ ablehnen.

```{r}
testing_2 <- slopes %>%
  mutate(
    ## Logical AND: &
    wrong_decision_blue = t_statistic0 < 2 & t_statistic0 > -2,
    wrong_decision_red = t_statistic0 < 3 & t_statistic0 > -3
  )
## Share of TRUE values
mean(testing_2$wrong_decision_blue)
mean(testing_2$wrong_decision_red)
```

## Wahl von kritischen Werten

Konvention: wähle die kritischen Werte so, daß die Wahrscheinlichkeit eine
wahre Nullhypothese abzulehnen gleich 5% ist.

# t-distribution

$$
y_i = \beta_0 + \beta_1 x_i + u_i
$$

Under some assumptions it can be shown that under the null hypothesis (this simply means that we assume the null hypothesis is true)

$$
H_0: \beta_1 = \beta_{1,H_0}\\
H_1: \beta_1 \neq \beta_{1,H_0}\\
$$

$$
\text{t-statistic} = \frac{\hat{\beta_1} -  \beta_{H_0}}{SE(\hat{\beta}_1)}
$$

$$
\text{t-statistic} \underbrace{\sim}_{H_0} t(\text{df} = n - p)
$$
Die t-Verteilung hat einen Parameter: (d)egrees of (f)reedom, Freiheitsgrade.


The t-statistic follow a t-distribution with $n - p$ degrees of freedom (parameter of the distribution), where $n$ is the number of observations in the linear model (in our example $n = 434$ kids) and $p$ is the number of coefficients in the linear equation. In our linear regression model the number of coefficients in $p = 2$: the intercept $\beta_0$, and the slope coefficient $\beta_1$.

## Probabilities and quantiles of the t-distribution


```{r}
sim_coeff %>%
  ggplot(aes(x = t_statistic)) +
    # geom_density(color = "steelblue4") +
    stat_function(fun = dt, n = 100, args = list(df = nrow(kids) - 2)) +
    labs(
      x = "x"
    ) +
    # geom_vline(xintercept = c(-2, 2), lty = 2)
    geom_vline(xintercept = c(-1.965471, 1.965471), lty = 2) +
    # geom_vline(xintercept = c(-1.648388, 1.648388), lty = 2) +
    # geom_vline(xintercept = c(-2.587258, 2.587258), lty = 2) +
    scale_x_continuous(breaks = c(-1.96, 1.96)) +
    xlim(c(-3, 3))
```
### Probability

```{r}
# p: probability, t: t-distribution
pt(-2, df = 2)
```


```{r}
# r: random, t: t-distribution
mean(rt(1000000, df = 2) < -2)
# mean(rt(1000000, df = 2) < -2)
```

### Quantiles

```{r}
## q: quantile, t: t-distribution
qt(p = 0.09175171, df = 2)
```


## Critical values in t-tests

```{r}
# 0.025 quantile of the t-distribution with 2 degrees of freedom
qt(0.025, df = 2)
```

```{r}
# r: random, t: t-distribution
mean(rt(1000000, df = 2) < -4.302653)
```

A convention is to use a 5% error probability of rejecting a true null hypothesis, 
so we use the quantiles of the t-distribution to derive critical values as follows:

```{r}
## Lower critical value: the 0.025 quantile of the t-distribution
qt(0.025, df = 2)

## Upper critical value: the 0.975 quantile of the t-distribution
qt(1 - 0.025, df = 2)
```
